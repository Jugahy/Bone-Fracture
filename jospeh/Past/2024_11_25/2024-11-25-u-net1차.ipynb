{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8cea365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 10:48:32.729355: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 10:48:32.774091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 10:48:33.541174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-25 10:48:34.752826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20169 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:86:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "# import tensorflow as tf2\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "config = tf1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "session = tf1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7142ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd036d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 이미지 데이터 크기: (737, 512, 512)\n",
      "훈련 레이블 데이터 크기: (737, 512, 512, 2)\n",
      "검증 이미지 데이터 크기: (185, 512, 512)\n",
      "검증 레이블 데이터 크기: (185, 512, 512, 2)\n"
     ]
    }
   ],
   "source": [
    "# .npy 파일 경로\n",
    "x_train_path = 'x_train.npy'\n",
    "y_train_path = 'y_train.npy'\n",
    "x_val_path = 'x_val.npy'\n",
    "y_val_path = 'y_val.npy'\n",
    "\n",
    "# 데이터 불러오기\n",
    "x_train = np.load(x_train_path)  # 훈련 이미지 데이터\n",
    "y_train = np.load(y_train_path)  # 훈련 레이블 데이터\n",
    "x_val = np.load(x_val_path)      # 검증 이미지 데이터\n",
    "y_val = np.load(y_val_path)      # 검증 레이블 데이터\n",
    "\n",
    "# 데이터 확인 (첫 번째 샘플 출력 예시)\n",
    "print(\"훈련 이미지 데이터 크기:\", x_train.shape)\n",
    "print(\"훈련 레이블 데이터 크기:\", y_train.shape)\n",
    "print(\"검증 이미지 데이터 크기:\", x_val.shape)\n",
    "print(\"검증 레이블 데이터 크기:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "652df838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    \"\"\"\n",
    "    훈련 및 검증 정확도와 손실을 시각화합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - history: 정확도 및 손실 값을 포함하는 훈련 기록 객체.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # 정확도 그래프\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Model Accuracy')\n",
    "\n",
    "    # 손실 그래프\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Model Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_f1_score(model, x_val, y_val, batch_size=4):\n",
    "    \"\"\"\n",
    "    F1 점수를 계산하고 출력합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 검증 데이터 세트에서 예측을 생성할 훈련된 모델.\n",
    "    - x_val: 검증 데이터 입력값.\n",
    "    - y_val: 검증 데이터 레이블.\n",
    "    - batch_size: 예측을 위한 배치 크기(기본값: 4).\n",
    "    \n",
    "    반환값:\n",
    "    - y_pred: 예측된 레이블.\n",
    "    - y_true: 실제 레이블.\n",
    "    \"\"\"\n",
    "    y_pred = np.argmax(model.predict(x_val, batch_size=batch_size), axis=-1)\n",
    "    y_true = np.argmax(y_val, axis=-1)\n",
    "\n",
    "    f1 = f1_score(y_true.flatten(), y_pred.flatten(), average='weighted')\n",
    "    print(f\"F1 점수: {f1:.2f}\")\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "def plot_confusion_matrix_with_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    혼동 행렬을 시각화하고 정밀도, 재현율, F1 점수를 출력합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - y_true: 실제 레이블.\n",
    "    - y_pred: 예측된 레이블.\n",
    "    \"\"\"\n",
    "    # Flatten 데이터\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # 혼동 행렬 계산\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # 메트릭 계산 (zero_division=0 설정)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # 메트릭 출력\n",
    "    print(\"=== 성능 지표 ===\")\n",
    "    print(f\"정밀도(Precision): {precision:.2f}\")\n",
    "    print(f\"재현율(Recall): {recall:.2f}\")\n",
    "    \n",
    "    # 혼동 행렬 시각화\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_true_vs_pred_images(model, x_val, y_val, num_images=5, batch_size=4):\n",
    "    \"\"\"\n",
    "    실제 이미지, 라벨 이미지 및 예측된 이미지를 나란히 시각화합니다.\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 검증 데이터 세트에서 예측을 생성할 훈련된 모델.\n",
    "    - x_val: 검증 데이터 입력값.\n",
    "    - y_val: 검증 데이터 레이블.\n",
    "    - num_images: 시각화할 이미지 수 (기본값: 5).\n",
    "    - batch_size: 예측을 위한 배치 크기(기본값: 4).\n",
    "    \"\"\"\n",
    "    # 예측 생성\n",
    "    y_pred = model.predict(x_val, batch_size=batch_size)\n",
    "    \n",
    "    plt.figure(figsize=(25, 3 * num_images))\n",
    "    for i in range(num_images):\n",
    "        # 실제 입력 이미지\n",
    "        plt.subplot(num_images, 5, 5 * i + 1)\n",
    "        plt.imshow(x_val[i].squeeze(), cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 실제 라벨 이미지 (첫 번째 채널 선택 - 전경)\n",
    "        plt.subplot(num_images, 5, 5 * i + 2)\n",
    "        plt.imshow(y_val[i][..., 1].squeeze(), cmap='gray')  # 전경 채널 시각화\n",
    "        plt.title('Label Image (Foreground)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 예측된 이미지 (전체 클래스 비교 - argmax로 클래스 라벨 선택)\n",
    "        plt.subplot(num_images, 5, 5 * i + 3)\n",
    "        plt.imshow(np.argmax(y_val[i], axis=-1).squeeze(), cmap='gray')\n",
    "        plt.title('Original Label (Argmax)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 예측된 이미지 (첫 번째 채널 선택 - 전경)\n",
    "        plt.subplot(num_images, 5, 5 * i + 4)\n",
    "        plt.imshow(y_pred[i][..., 1].squeeze(), cmap='gray')  # 전경 채널 시각화\n",
    "        plt.title('Predicted Image (Foreground)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 예측된 이미지 (전체 클래스 비교 - argmax로 클래스 라벨 선택)\n",
    "        plt.subplot(num_images, 5, 5 * i + 5)\n",
    "        plt.imshow(np.argmax(y_pred[i], axis=-1).squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Label (Argmax)')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def main_display_result(history, model, x_val, y_val, batch_size=4, num_images=5):\n",
    "    \"\"\"\n",
    "    전체 워크플로우 실행: 정확도/손실 그래프 시각화, F1 점수 계산, 혼동 행렬 및 메트릭 출력, 실제/예측 이미지 비교.\n",
    "    \n",
    "    매개변수:\n",
    "    - history: 정확도 및 손실 값을 포함하는 훈련 기록 객체.\n",
    "    - model: 검증 데이터 세트에서 예측을 생성할 훈련된 모델.\n",
    "    - x_val: 검증 데이터 입력값.\n",
    "    - y_val: 검증 데이터 레이블.\n",
    "    - batch_size: 예측을 위한 배치 크기(기본값: 4).\n",
    "    - num_images: 시각화할 이미지 수 (기본값: 5).\n",
    "    \"\"\"\n",
    "    # 훈련 및 검증 정확도/손실 그래프 시각화\n",
    "    plot_accuracy_loss(history)\n",
    "    \n",
    "    # F1 점수 계산 및 예측값 반환\n",
    "    y_true, y_pred = calculate_f1_score(model, x_val, y_val, batch_size=batch_size)\n",
    "    \n",
    "    # 혼동 행렬 및 메트릭 출력\n",
    "    plot_confusion_matrix_with_metrics(y_true, y_pred)\n",
    "    \n",
    "    # 실제 이미지와 예측된 이미지 비교 시각화\n",
    "    plot_true_vs_pred_images(model, x_val, y_val, num_images=num_images, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdec043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from deap import base, creator, tools\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Cropping2D\n",
    "\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "\n",
    "def unet_model_dynamic(input_shape, num_classes, num_layers, filters_per_layer):\n",
    "    # 입력 크기 정수화\n",
    "    input_shape = tuple(map(int, input_shape))  # Ensure input_shape is integer\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoder_layers = []\n",
    "    x = inputs\n",
    "    for i in range(num_layers):\n",
    "        x = Conv2D(filters_per_layer[i], (3, 3), activation='relu', padding='same')(x)\n",
    "        x = Conv2D(filters_per_layer[i], (3, 3), activation='relu', padding='same')(x)\n",
    "        encoder_layers.append(x)\n",
    "        if i < num_layers - 1:\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = Conv2D(filters_per_layer[-1], (3, 3), activation='relu', padding='same')(x)\n",
    "    bottleneck = Conv2D(filters_per_layer[-1], (3, 3), activation='relu', padding='same')(bottleneck)\n",
    "\n",
    "    # Decoder\n",
    "    x = bottleneck\n",
    "    for i in range(num_layers - 1, -1, -1):\n",
    "        x = Conv2DTranspose(filters_per_layer[i], (2, 2), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "        # 크기 조정\n",
    "        diff_height = encoder_layers[i].shape[1] - x.shape[1]\n",
    "        diff_width = encoder_layers[i].shape[2] - x.shape[2]\n",
    "\n",
    "        # 정수형 변환\n",
    "        diff_height = int(diff_height)\n",
    "        diff_width = int(diff_width)\n",
    "\n",
    "        # ZeroPadding2D 또는 Cropping2D를 이용하여 크기 조정\n",
    "        if diff_height > 0 or diff_width > 0:\n",
    "            x = ZeroPadding2D(((diff_height // 2, diff_height - diff_height // 2),\n",
    "                               (diff_width // 2, diff_width - diff_width // 2)))(x)\n",
    "        elif diff_height < 0 or diff_width < 0:\n",
    "            x = Cropping2D(((-diff_height // 2, -diff_height + diff_height // 2),\n",
    "                            (-diff_width // 2, -diff_width + diff_width // 2)))(x)\n",
    "\n",
    "        x = concatenate([x, encoder_layers[i]])\n",
    "        x = Conv2D(filters_per_layer[i], (3, 3), activation='relu', padding='same')(x)\n",
    "        x = Conv2D(filters_per_layer[i], (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ddcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패치 단위 학습 함수 (필요시 수정 가능)\n",
    "def train_unet_with_patches(model, x_train, y_train, x_val, y_val, patch_size, batch_size, epochs):\n",
    "    return model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2a46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적합도 평가 함수\n",
    "def evaluate(individual):\n",
    "    lr, batch_size, patch_size, num_layers, *filters_per_layer = individual\n",
    "\n",
    "    # 필터 크기를 정수로 변환\n",
    "    filters_per_layer = [int(f) for f in filters_per_layer]\n",
    "\n",
    "    # 모델 생성\n",
    "    model = unet_model_dynamic(\n",
    "        input_shape=(patch_size, patch_size, 1),\n",
    "        num_classes=2,\n",
    "        num_layers=num_layers,\n",
    "        filters_per_layer=filters_per_layer\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # 패치 단위 학습\n",
    "    history = train_unet_with_patches(\n",
    "        model,\n",
    "        x_train, y_train, x_val, y_val,\n",
    "        patch_size=(patch_size, patch_size),\n",
    "        batch_size=batch_size,\n",
    "        epochs=3\n",
    "    )\n",
    "\n",
    "    val_loss = history.history[\"val_loss\"][-1]\n",
    "    return val_loss,\n",
    "\n",
    "# 유전 알고리즘 설정\n",
    "param_ranges = {\n",
    "    \"learning_rate\": (1e-5, 1e-2),\n",
    "    \"batch_size\": (4, 16),\n",
    "    \"patch_size\": (32, 128),\n",
    "    \"num_layers\": (3, 5),\n",
    "    \"filters\": (32, 128)\n",
    "}\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_lr\", random.uniform, *param_ranges[\"learning_rate\"])\n",
    "toolbox.register(\"attr_batch_size\", random.randint, *param_ranges[\"batch_size\"])\n",
    "toolbox.register(\"attr_patch_size\", random.randint, *param_ranges[\"patch_size\"])\n",
    "toolbox.register(\"attr_num_layers\", random.randint, *param_ranges[\"num_layers\"])\n",
    "toolbox.register(\"attr_filters\", random.randint, *param_ranges[\"filters\"])\n",
    "\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_lr, toolbox.attr_batch_size, toolbox.attr_patch_size,\n",
    "                  toolbox.attr_num_layers) + (toolbox.attr_filters,) * param_ranges[\"num_layers\"][1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59b774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 10:48:37.668735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20169 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:86:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 8, 8, 103), (None, 7, 7, 103)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m---> 23\u001b[0m         ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m \u001b[43mtoolbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m fits \u001b[38;5;241m=\u001b[39m [ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest fitness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(fits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [7], line 9\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m      6\u001b[0m filters_per_layer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filters_per_layer]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 모델 생성\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43munet_model_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters_per_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters_per_layer\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn [5], line 52\u001b[0m, in \u001b[0;36munet_model_dynamic\u001b[0;34m(input_shape, num_classes, num_layers, filters_per_layer)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m diff_height \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m diff_width \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m Cropping2D(((\u001b[38;5;241m-\u001b[39mdiff_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39mdiff_height \u001b[38;5;241m+\u001b[39m diff_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     50\u001b[0m                     (\u001b[38;5;241m-\u001b[39mdiff_width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39mdiff_width \u001b[38;5;241m+\u001b[39m diff_width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)))(x)\n\u001b[0;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv2D(filters_per_layer[i], (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv2D(filters_per_layer[i], (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/src/layers/merging/concatenate.py:231\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;124;03m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/src/layers/merging/concatenate.py:131\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    125\u001b[0m unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    126\u001b[0m     shape[axis]\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 8, 8, 103), (None, 7, 7, 103)]"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "population = toolbox.population(n=10)\n",
    "NGEN = 5\n",
    "for gen in range(NGEN):\n",
    "    print(f\"Generation {gen}\")\n",
    "    offspring = tools.selBest(population, k=len(population)//2)\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random.random() < 0.5:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values, child2.fitness.values\n",
    "\n",
    "    for mutant in offspring:\n",
    "        if random.random() < 0.2:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "    population[:] = offspring\n",
    "\n",
    "    for ind in population:\n",
    "        if not ind.fitness.valid:\n",
    "            ind.fitness.values = toolbox.evaluate(ind)\n",
    "\n",
    "    fits = [ind.fitness.values[0] for ind in population]\n",
    "    print(f\"Best fitness: {min(fits)}\")\n",
    "\n",
    "# 최적 개체 확인\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "print(\"Best Individual:\", best_individual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
