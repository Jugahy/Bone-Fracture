{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8cea365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 00:53:25.592339: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 00:53:25.737708: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-05 00:53:25.763945: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 00:53:26.280631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-12-05 00:53:26.280691: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-12-05 00:53:26.280696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-12-05 00:53:26.953603: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 00:53:27.104542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24217 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:19:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "# import tensorflow as tf2\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "session = tf1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa099c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class SPP_NET(nn.Module):\n",
    "    def __init__(self, opt, input_nc, ndf=64, gpu_ids=[]):\n",
    "        super(SPP_NET, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.output_num = [4, 2, 1]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.LReLU1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 1, 1, bias=False)\n",
    "        self.BN1 = nn.BatchNorm2d(ndf * 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 1, 1, bias=False)\n",
    "        self.BN2 = nn.BatchNorm2d(ndf * 4)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 1, 1, bias=False)\n",
    "        self.BN3 = nn.BatchNorm2d(ndf * 8)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, 64, 4, 1, 0, bias=False)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(86016, 4096)  # Adjust this size to match spp output size\n",
    "        self.fc2 = nn.Linear(4096, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device  # 텐서가 있는 장치 정보 가져오기\n",
    "\n",
    "        # Conv Layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.LReLU1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.LReLU1(self.BN1(x))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.LReLU1(self.BN2(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        # SPP Layer\n",
    "        spp = self.spatial_pyramid_pool(x, 1, [int(x.size(2)), int(x.size(3))], self.output_num)\n",
    "        \n",
    "        # Check the shape of spp\n",
    "        print(\"SPP shape:\", spp.shape)  # 확인\n",
    "\n",
    "        # Fully connected layers\n",
    "        fc1 = self.fc1(spp.view(spp.size(0), -1))  # Flatten the output before passing to fc1\n",
    "        fc2 = self.fc2(fc1)\n",
    "\n",
    "        s = nn.Sigmoid()\n",
    "        output = s(fc2)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def spatial_pyramid_pool(self, previous_conv, num_sample, previous_conv_size, out_pool_size):\n",
    "        '''Define spatial pyramid pooling here'''\n",
    "        \n",
    "        # GPU로 텐서를 이동\n",
    "        device = previous_conv.device\n",
    "    \n",
    "        spp = None\n",
    "        for i in range(len(out_pool_size)):\n",
    "            h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size[i]))\n",
    "            w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size[i]))\n",
    "            h_pad = int((h_wid * out_pool_size[i] - previous_conv_size[0] + 1) / 2)\n",
    "            w_pad = int((w_wid * out_pool_size[i] - previous_conv_size[1] + 1) / 2)\n",
    "            \n",
    "            maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(h_pad, w_pad))\n",
    "            x = maxpool(previous_conv)\n",
    "            \n",
    "            if i == 0:\n",
    "                spp = x.view(num_sample, -1)\n",
    "            else:\n",
    "                spp = torch.cat((spp, x.view(num_sample, -1)), 1)\n",
    "        \n",
    "        return spp.to(device)  # 최종 출력도 동일한 장치로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b575657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPP shape: torch.Size([1, 86016])\n",
      "Output shape: torch.Size([1, 1000])\n",
      "Output: tensor([[0.5759, 0.5669, 0.4536, 0.3816, 0.4889, 0.6272, 0.5306, 0.5080, 0.5401,\n",
      "         0.3916, 0.4387, 0.6181, 0.4787, 0.5763, 0.4875, 0.5882, 0.4196, 0.4126,\n",
      "         0.4589, 0.3978, 0.4634, 0.5243, 0.5375, 0.5669, 0.4812, 0.5593, 0.5858,\n",
      "         0.6218, 0.2366, 0.4702, 0.5190, 0.5762, 0.5054, 0.5091, 0.4384, 0.5772,\n",
      "         0.4983, 0.4799, 0.5230, 0.5686, 0.3098, 0.5304, 0.7188, 0.4653, 0.6383,\n",
      "         0.4807, 0.5588, 0.5048, 0.5314, 0.6031, 0.3872, 0.4631, 0.4147, 0.3740,\n",
      "         0.3253, 0.5423, 0.4415, 0.3990, 0.3955, 0.3746, 0.4197, 0.4216, 0.4979,\n",
      "         0.5411, 0.5646, 0.4923, 0.6237, 0.4366, 0.3955, 0.5133, 0.4900, 0.5011,\n",
      "         0.4818, 0.5194, 0.3149, 0.6630, 0.4230, 0.6218, 0.4500, 0.5959, 0.3553,\n",
      "         0.5351, 0.3687, 0.5671, 0.6232, 0.4131, 0.4010, 0.4782, 0.4079, 0.4814,\n",
      "         0.4878, 0.4547, 0.5461, 0.3632, 0.4229, 0.5846, 0.4360, 0.5362, 0.4121,\n",
      "         0.4777, 0.5558, 0.5931, 0.6332, 0.5473, 0.4488, 0.5320, 0.4760, 0.5011,\n",
      "         0.6089, 0.6291, 0.6353, 0.4651, 0.4796, 0.5071, 0.5305, 0.5668, 0.3383,\n",
      "         0.4869, 0.4663, 0.4974, 0.6211, 0.6073, 0.5720, 0.5990, 0.6305, 0.3982,\n",
      "         0.3644, 0.5223, 0.4996, 0.4939, 0.3198, 0.4448, 0.4374, 0.5400, 0.4590,\n",
      "         0.4612, 0.5891, 0.4999, 0.4102, 0.5299, 0.4171, 0.6605, 0.4832, 0.4230,\n",
      "         0.4998, 0.5838, 0.5300, 0.6255, 0.3350, 0.3692, 0.4189, 0.3113, 0.5545,\n",
      "         0.4467, 0.6810, 0.3193, 0.4679, 0.4320, 0.5773, 0.4971, 0.5547, 0.3710,\n",
      "         0.3655, 0.6079, 0.5229, 0.4506, 0.6073, 0.6389, 0.5322, 0.4910, 0.4335,\n",
      "         0.4696, 0.5077, 0.4997, 0.5553, 0.3159, 0.3870, 0.4227, 0.3700, 0.6695,\n",
      "         0.4877, 0.4019, 0.4451, 0.5023, 0.4575, 0.5682, 0.3644, 0.6467, 0.2632,\n",
      "         0.2350, 0.6228, 0.4086, 0.4464, 0.4901, 0.4355, 0.4508, 0.3436, 0.4621,\n",
      "         0.5037, 0.6193, 0.5161, 0.2485, 0.3648, 0.5174, 0.4215, 0.5311, 0.5307,\n",
      "         0.4051, 0.3728, 0.4341, 0.6077, 0.3901, 0.5139, 0.6023, 0.5544, 0.6712,\n",
      "         0.4195, 0.5584, 0.5000, 0.4767, 0.5147, 0.3859, 0.4934, 0.5192, 0.4139,\n",
      "         0.5629, 0.6146, 0.5377, 0.6353, 0.4441, 0.6186, 0.5185, 0.3052, 0.4155,\n",
      "         0.3601, 0.5275, 0.3762, 0.4761, 0.5002, 0.4932, 0.4446, 0.4643, 0.5699,\n",
      "         0.5916, 0.4286, 0.4510, 0.5807, 0.5864, 0.2526, 0.6181, 0.5582, 0.5015,\n",
      "         0.4227, 0.3689, 0.5251, 0.4611, 0.3939, 0.6135, 0.4687, 0.4715, 0.7106,\n",
      "         0.6078, 0.6071, 0.6368, 0.5758, 0.4216, 0.6480, 0.4322, 0.5801, 0.4831,\n",
      "         0.4870, 0.4421, 0.6300, 0.6042, 0.3833, 0.6853, 0.4720, 0.5847, 0.4915,\n",
      "         0.5563, 0.2948, 0.4993, 0.6507, 0.5135, 0.5596, 0.6010, 0.4235, 0.4332,\n",
      "         0.4508, 0.5253, 0.5864, 0.3292, 0.4680, 0.4584, 0.5394, 0.6461, 0.4063,\n",
      "         0.5326, 0.3398, 0.6559, 0.4759, 0.5950, 0.5421, 0.4752, 0.4010, 0.4634,\n",
      "         0.5787, 0.5356, 0.6108, 0.4956, 0.4857, 0.3856, 0.4150, 0.6132, 0.5669,\n",
      "         0.5630, 0.4441, 0.3970, 0.3390, 0.4899, 0.5110, 0.4341, 0.6004, 0.5906,\n",
      "         0.3602, 0.4273, 0.5046, 0.5034, 0.4610, 0.3509, 0.6495, 0.5722, 0.4377,\n",
      "         0.4478, 0.3730, 0.5294, 0.5276, 0.5433, 0.3704, 0.5041, 0.5843, 0.5767,\n",
      "         0.5598, 0.4245, 0.5759, 0.5040, 0.5839, 0.5838, 0.6226, 0.3793, 0.4418,\n",
      "         0.3890, 0.6087, 0.5124, 0.5657, 0.5093, 0.5123, 0.4500, 0.4399, 0.4029,\n",
      "         0.3975, 0.4283, 0.4100, 0.5564, 0.5376, 0.4899, 0.5063, 0.6033, 0.5937,\n",
      "         0.3351, 0.4556, 0.4760, 0.5521, 0.4832, 0.4152, 0.4858, 0.5615, 0.5725,\n",
      "         0.5002, 0.3803, 0.6118, 0.4902, 0.4588, 0.3617, 0.5177, 0.4118, 0.6360,\n",
      "         0.4486, 0.6069, 0.5787, 0.6598, 0.3892, 0.5110, 0.5724, 0.5682, 0.4598,\n",
      "         0.4807, 0.5025, 0.5332, 0.4795, 0.5012, 0.6369, 0.6074, 0.4906, 0.4564,\n",
      "         0.3816, 0.3614, 0.4985, 0.4428, 0.4040, 0.6202, 0.5746, 0.5000, 0.4680,\n",
      "         0.4020, 0.5688, 0.5776, 0.6220, 0.5094, 0.5406, 0.4335, 0.5097, 0.4361,\n",
      "         0.3880, 0.5508, 0.6401, 0.5236, 0.4320, 0.6036, 0.3971, 0.6869, 0.5815,\n",
      "         0.4866, 0.6109, 0.5686, 0.5766, 0.3737, 0.2709, 0.3545, 0.4574, 0.6057,\n",
      "         0.5733, 0.5607, 0.5047, 0.5620, 0.3569, 0.3289, 0.6403, 0.4002, 0.4071,\n",
      "         0.5620, 0.4251, 0.6033, 0.5539, 0.5667, 0.5203, 0.3745, 0.6115, 0.6201,\n",
      "         0.6472, 0.4388, 0.5595, 0.4130, 0.5923, 0.3858, 0.5658, 0.4642, 0.3509,\n",
      "         0.5386, 0.4742, 0.6725, 0.3522, 0.4396, 0.3101, 0.5357, 0.4751, 0.4166,\n",
      "         0.4468, 0.5542, 0.5453, 0.4268, 0.5271, 0.5585, 0.5101, 0.4641, 0.3784,\n",
      "         0.5501, 0.6221, 0.3815, 0.5474, 0.5658, 0.5014, 0.5603, 0.4826, 0.3172,\n",
      "         0.5456, 0.5300, 0.4340, 0.6303, 0.3576, 0.6060, 0.3107, 0.2995, 0.5533,\n",
      "         0.5666, 0.4007, 0.6240, 0.6002, 0.4536, 0.4526, 0.3833, 0.3504, 0.3932,\n",
      "         0.4058, 0.4612, 0.4620, 0.4463, 0.4860, 0.4558, 0.5241, 0.5805, 0.5613,\n",
      "         0.4487, 0.5259, 0.4550, 0.4926, 0.4500, 0.4606, 0.3804, 0.4981, 0.4401,\n",
      "         0.5254, 0.4196, 0.5170, 0.5602, 0.3381, 0.4250, 0.4926, 0.4801, 0.5433,\n",
      "         0.4773, 0.3803, 0.5116, 0.6438, 0.5298, 0.5954, 0.6907, 0.4568, 0.4446,\n",
      "         0.5768, 0.4776, 0.4837, 0.5017, 0.5948, 0.6122, 0.5391, 0.2895, 0.4643,\n",
      "         0.6079, 0.5092, 0.6535, 0.5265, 0.6696, 0.4291, 0.6224, 0.6237, 0.3862,\n",
      "         0.5089, 0.5829, 0.3996, 0.6447, 0.3539, 0.4449, 0.5999, 0.6181, 0.4044,\n",
      "         0.3629, 0.6293, 0.3204, 0.6191, 0.3069, 0.4881, 0.2765, 0.2982, 0.6011,\n",
      "         0.4246, 0.5096, 0.6383, 0.4961, 0.4200, 0.5261, 0.4565, 0.6176, 0.5750,\n",
      "         0.3950, 0.7468, 0.4720, 0.4191, 0.5080, 0.5076, 0.5821, 0.4922, 0.5172,\n",
      "         0.4721, 0.5347, 0.3650, 0.5512, 0.5815, 0.6295, 0.4458, 0.5453, 0.5693,\n",
      "         0.3874, 0.4215, 0.4515, 0.6118, 0.4982, 0.5093, 0.5822, 0.4207, 0.3455,\n",
      "         0.4820, 0.6609, 0.6105, 0.6342, 0.6107, 0.4403, 0.4578, 0.4566, 0.5160,\n",
      "         0.3422, 0.6347, 0.3509, 0.5673, 0.6416, 0.5458, 0.6225, 0.5036, 0.5140,\n",
      "         0.4656, 0.5695, 0.6289, 0.6264, 0.4952, 0.4060, 0.5226, 0.5030, 0.4961,\n",
      "         0.6243, 0.6287, 0.4648, 0.4204, 0.4250, 0.5052, 0.3945, 0.4507, 0.6591,\n",
      "         0.5778, 0.5020, 0.6378, 0.4652, 0.6591, 0.6617, 0.4598, 0.3493, 0.6392,\n",
      "         0.3847, 0.4500, 0.5371, 0.5842, 0.4603, 0.5306, 0.3447, 0.5888, 0.4996,\n",
      "         0.5274, 0.5729, 0.5618, 0.2382, 0.4587, 0.4280, 0.5100, 0.3744, 0.4072,\n",
      "         0.5509, 0.4553, 0.5305, 0.5415, 0.4313, 0.5798, 0.4708, 0.3125, 0.4456,\n",
      "         0.5932, 0.4589, 0.4281, 0.5821, 0.4148, 0.4591, 0.5312, 0.4183, 0.5342,\n",
      "         0.5007, 0.4425, 0.5812, 0.4982, 0.3517, 0.5617, 0.4930, 0.5749, 0.3820,\n",
      "         0.5538, 0.4964, 0.6295, 0.4817, 0.3202, 0.5271, 0.5501, 0.4365, 0.5042,\n",
      "         0.5189, 0.4795, 0.6111, 0.6054, 0.5065, 0.4789, 0.4429, 0.3611, 0.4358,\n",
      "         0.4156, 0.5028, 0.5101, 0.4883, 0.5009, 0.5265, 0.5332, 0.4692, 0.4733,\n",
      "         0.4605, 0.4743, 0.5849, 0.3721, 0.4447, 0.4052, 0.6269, 0.5908, 0.3041,\n",
      "         0.4005, 0.6321, 0.4601, 0.5025, 0.6173, 0.4125, 0.6056, 0.3486, 0.5332,\n",
      "         0.4819, 0.4599, 0.5583, 0.5176, 0.6098, 0.4937, 0.5338, 0.5506, 0.5058,\n",
      "         0.4063, 0.4254, 0.5405, 0.5599, 0.4159, 0.4645, 0.5150, 0.5910, 0.4922,\n",
      "         0.5500, 0.5114, 0.4168, 0.3487, 0.3838, 0.4001, 0.4873, 0.5178, 0.4790,\n",
      "         0.5540, 0.5781, 0.5889, 0.4277, 0.5983, 0.5556, 0.4715, 0.7098, 0.5175,\n",
      "         0.4172, 0.6627, 0.6710, 0.5029, 0.4425, 0.4881, 0.6079, 0.5901, 0.4674,\n",
      "         0.5099, 0.6754, 0.4506, 0.5702, 0.6570, 0.4792, 0.6111, 0.4418, 0.4731,\n",
      "         0.5191, 0.3730, 0.4547, 0.5055, 0.6396, 0.4718, 0.4106, 0.4533, 0.4659,\n",
      "         0.4591, 0.4542, 0.2960, 0.3891, 0.3361, 0.5259, 0.6362, 0.4429, 0.3927,\n",
      "         0.5820, 0.6194, 0.4511, 0.5140, 0.6765, 0.5704, 0.4234, 0.5734, 0.5800,\n",
      "         0.4593, 0.5678, 0.4898, 0.6312, 0.4178, 0.2363, 0.4932, 0.5770, 0.4192,\n",
      "         0.5626, 0.5726, 0.5582, 0.4052, 0.4930, 0.3463, 0.4854, 0.5392, 0.4658,\n",
      "         0.5749, 0.4831, 0.5613, 0.3982, 0.4673, 0.4697, 0.6089, 0.5994, 0.6714,\n",
      "         0.4499, 0.5502, 0.4470, 0.5177, 0.4762, 0.5028, 0.5790, 0.4026, 0.4384,\n",
      "         0.5650, 0.6053, 0.5122, 0.4397, 0.2231, 0.5567, 0.5079, 0.5765, 0.2652,\n",
      "         0.5287, 0.4264, 0.4939, 0.4742, 0.5112, 0.4783, 0.4375, 0.3759, 0.4746,\n",
      "         0.4214, 0.6051, 0.5024, 0.5821, 0.4185, 0.4987, 0.4413, 0.6603, 0.3749,\n",
      "         0.6359, 0.5226, 0.5177, 0.4903, 0.5484, 0.4319, 0.4514, 0.6237, 0.2762,\n",
      "         0.4750, 0.5885, 0.5858, 0.3455, 0.5673, 0.6474, 0.3840, 0.5700, 0.4503,\n",
      "         0.5602, 0.6092, 0.4934, 0.5312, 0.6475, 0.5505, 0.5883, 0.3751, 0.5047,\n",
      "         0.3655, 0.3747, 0.4574, 0.3062, 0.4424, 0.5464, 0.4228, 0.3523, 0.4373,\n",
      "         0.4645, 0.5845, 0.4661, 0.5344, 0.3638, 0.4425, 0.4301, 0.5275, 0.5075,\n",
      "         0.4582, 0.4715, 0.4308, 0.4204, 0.4474, 0.6617, 0.5304, 0.3623, 0.6934,\n",
      "         0.4124, 0.4874, 0.3820, 0.5637, 0.4073, 0.4713, 0.5345, 0.4468, 0.3544,\n",
      "         0.4862, 0.5368, 0.6228, 0.6023, 0.6505, 0.5320, 0.4052, 0.5314, 0.4564,\n",
      "         0.6008, 0.6386, 0.4556, 0.4628, 0.5477, 0.6156, 0.5466, 0.5604, 0.5913,\n",
      "         0.6454, 0.4970, 0.6548, 0.2588, 0.4287, 0.5110, 0.6468, 0.6103, 0.4163,\n",
      "         0.5359, 0.3458, 0.4694, 0.4231, 0.4460, 0.6099, 0.6584, 0.3350, 0.4885,\n",
      "         0.4579]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# 입력 이미지 크기 설정\n",
    "batch_size = 8\n",
    "input_channels = 3  # 예를 들어, RGB 이미지\n",
    "image_height = 1000\n",
    "image_width = 2500\n",
    "\n",
    "# 입력 텐서 생성 (배치 크기 8, 3 채널, 64x64 크기의 이미지)\n",
    "input_tensor = torch.randn(batch_size, input_channels, image_height, image_width)\n",
    "\n",
    "# 모델 초기화\n",
    "opt = None  # 옵션은 현재 코드에서 사용되지 않으므로 None으로 설정\n",
    "model = SPP_NET(opt, input_nc=input_channels)\n",
    "\n",
    "# 모델과 입력 텐서를 동일한 장치로 이동시키기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 device로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 입력 텐서를 device로 이동\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# 모델 실행\n",
    "output = model(input_tensor)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Output shape:\", output.shape)  # 예측된 출력 텐서의 크기\n",
    "print(\"Output:\", output)  # 예측된 출력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff78e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
